{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Classify language out of the list given below using just stop words. Remove punctuations, make lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'bengali',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test=\"An article is qualunque member van un class of dedicated words naquele estão used with noun phrases per mark the identifiability of the referents of the noun phrases\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "{'arabic': 0,\n",
    " 'azerbaijani': 1,\n",
    " 'danish': 0,\n",
    " 'dutch': 3,\n",
    " 'english': 5,\n",
    " 'finnish': 0,\n",
    " 'french': 1,\n",
    " 'german': 1,\n",
    " 'greek': 0,\n",
    " 'hungarian': 1,\n",
    " 'indonesian': 1,\n",
    " 'italian': 2,\n",
    " 'kazakh': 0,\n",
    " 'nepali': 0,\n",
    " 'norwegian': 0,\n",
    " 'portuguese': 1,\n",
    " 'romanian': 1,\n",
    " 'romanurdu': 1,\n",
    " 'russian': 0,\n",
    " 'slovene': 0,\n",
    " 'spanish': 1,\n",
    " 'swedish': 0,\n",
    " 'tajik': 0,\n",
    " 'turkish': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arabic': 0,\n",
       " 'azerbaijani': 1,\n",
       " 'bengali': 0,\n",
       " 'danish': 0,\n",
       " 'dutch': 3,\n",
       " 'english': 5,\n",
       " 'finnish': 0,\n",
       " 'french': 1,\n",
       " 'german': 1,\n",
       " 'greek': 0,\n",
       " 'hungarian': 1,\n",
       " 'indonesian': 1,\n",
       " 'italian': 2,\n",
       " 'kazakh': 0,\n",
       " 'nepali': 0,\n",
       " 'norwegian': 0,\n",
       " 'portuguese': 1,\n",
       " 'romanian': 1,\n",
       " 'russian': 0,\n",
       " 'slovene': 0,\n",
       " 'spanish': 1,\n",
       " 'swedish': 0,\n",
       " 'tajik': 0,\n",
       " 'turkish': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "token = word_tokenize(Test)\n",
    "\n",
    "languages_ratios = {}\n",
    "words = [word.lower() for word in token]\n",
    "\n",
    "for language in stopwords.fileids():\n",
    "   set1 = set(stopwords.words(language))\n",
    "   words_set = set(words)\n",
    "   set2 = words_set.intersection(set1)\n",
    "   languages_ratios[language] = len(set2) # language \"score\"\n",
    "languages_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'english'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_rated_language = max(languages_ratios, key=languages_ratios.get)\n",
    "most_rated_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An',\n",
       " 'article',\n",
       " 'is',\n",
       " 'qualunque',\n",
       " 'member',\n",
       " 'van',\n",
       " 'un',\n",
       " 'class',\n",
       " 'of',\n",
       " 'dedicated',\n",
       " 'words',\n",
       " 'naquele',\n",
       " 'estão',\n",
       " 'used',\n",
       " 'with',\n",
       " 'noun',\n",
       " 'phrases',\n",
       " 'per',\n",
       " 'mark',\n",
       " 'the',\n",
       " 'identifiability',\n",
       " 'of',\n",
       " 'the',\n",
       " 'referents',\n",
       " 'of',\n",
       " 'the',\n",
       " 'noun',\n",
       " 'phrases']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "lang_token = word_tokenize(Test)\n",
    "lang_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question1 miscellaneous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lang_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'of': 3, 'the': 3, 'noun': 2, 'phrases': 2, 'an': 1, 'article': 1, 'is': 1, 'qualunque': 1, 'member': 1, 'van': 1, ...})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in lang_token:\n",
    "    fdist[word.lower()]+=1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist[\"noun\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 3),\n",
       " ('the', 3),\n",
       " ('noun', 2),\n",
       " ('phrases', 2),\n",
       " ('an', 1),\n",
       " ('article', 1),\n",
       " ('is', 1),\n",
       " ('qualunque', 1),\n",
       " ('member', 1),\n",
       " ('van', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10 = fdist.most_common(10)\n",
    "fdist_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ã'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "text_vocab = set(w.lower() for w in Test if w.lower().isalpha())\n",
    "unusual = text_vocab.difference(english_vocab)\n",
    "unusual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Make a Roman urdu sentence tokenizer by assuming that there will be no (.full stop) and (? question mark) in the end of sentence. Make some rules to made that tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test2=\"lekin daad dainee padtee hai snatkaron ko jo aisay bohat se digrmsayl ki terhan is maslay ka hal nikaalte mein bhi kamyaab ho gaye hain ghair mulki tehqeeqi jareeday fiction mein shaya honay wali report ke mutabiq snatkaron nay aik aisi battery tayaar karli hai jisay saal mein sirf 1 baar charge karna parre ga magar yeh battery kamray ke daraja hararat mein behtareen kaam kere gi is aylominim ion battery ko jet laboratory, larns national laboratory Nasa , hnda reserch insti tute ke mahireen par mushtamil team naay tayyar kya hai jisay aik karne ke baad taqreeban 10 Mahtaq dobarah charge karne ki zaroorat nahi rehti hai tehqeeqi team ke sarbarah aur 2008 hamza mein noble inaam haasil karne walay chemiya ke professor Robert  grbs ne bataya ke aylominim ion battery murawaja batrion ke muqablay mein 20 gina taaqat war hoti hain aur inhen 10 mah taq charge karne ki zaroorat nahi padtee ha team ke sarbarah professor Robert grace ke mutabiq aam istemaal ki jany wali batrion ke muqablay main zyada achi h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = string.punctuation\n",
    "punctuation\n",
    "import string\n",
    "from nltk import WordNetLemmatizer\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "punctuation = string.punctuation\n",
    "\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# test2_token= sent_tokenize(Test2)\n",
    "# test2_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words=['ai', 'ayi', 'hy', 'hai', 'main', 'ki', 'tha', 'koi', 'ko', 'sy', 'woh', 'bhi', 'aur', 'wo', 'yeh', 'rha', 'hota', 'ho', 'ga', 'ka', 'le', 'lye', 'kr', 'kar', 'lye', 'liye', 'hotay', 'waisay', 'gya', 'gaya', 'kch', 'ab', 'thy', 'thay', 'houn', 'hain', 'han', 'to', 'is', 'hi', 'jo', 'kya', 'thi', 'se', 'pe', 'phr', 'wala', 'waisay', 'us', 'na', 'ny', 'hun', 'ha', 'raha', 'ja', 'rahay', 'abi', 'uski', 'ne', 'haan', 'acha', 'nai', 'sent', 'photo', 'you', 'kafi', 'gai', 'rhy', 'kuch', 'jata', 'aye', 'ya', 'dono', 'hoa', 'aese', 'de', 'wohi', 'jati', 'jb', 'krta', 'lg', 'rahi', 'hui', 'karna', 'krna', 'gi', 'hova', 'yehi', 'jana', 'jye', 'chal', 'mil', 'tu', 'hum', 'par', 'hay', 'kis', 'sb', 'gy', 'dain', 'krny', 'tou']\n",
    "# stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lekin', 'NN')]\n",
      "[('daad', 'NN')]\n",
      "[('dainee', 'NN')]\n",
      "[('padtee', 'NN')]\n",
      "[('hai', 'NN')]\n",
      "[('snatkaron', 'NN')]\n",
      "[('ko', 'NN')]\n",
      "[('jo', 'NN')]\n",
      "[('aisay', 'NN')]\n",
      "[('bohat', 'IN')]\n",
      "[('se', 'NN')]\n",
      "[('digrmsayl', 'NN')]\n",
      "[('ki', 'NN')]\n",
      "[('terhan', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('maslay', 'NN')]\n",
      "[('ka', 'NN')]\n",
      "[('hal', 'NN')]\n",
      "[('nikaalte', 'NN')]\n",
      "[('mein', 'NN')]\n",
      "[('bhi', 'NN')]\n",
      "[('kamyaab', 'NN')]\n",
      "[('ho', 'NN')]\n",
      "[('gaye', 'NN')]\n",
      "[('hain', 'NN')]\n",
      "[('ghair', 'NN')]\n",
      "[('mulki', 'NN')]\n",
      "[('tehqeeqi', 'NN')]\n",
      "[('jareeday', 'NN')]\n",
      "[('fiction', 'NN')]\n",
      "[('mein', 'NN')]\n",
      "[('shaya', 'NN')]\n",
      "[('honay', 'NN')]\n",
      "[('wali', 'NN')]\n",
      "[('report', 'NN')]\n",
      "[('ke', 'NN')]\n",
      "[('mutabiq', 'NN')]\n",
      "[('snatkaron', 'NN')]\n",
      "[('nay', 'NN')]\n",
      "[('aik', 'NN')]\n",
      "[('aisi', 'NN')]\n",
      "[('battery', 'NN')]\n",
      "[('tayaar', 'NN')]\n",
      "[('karli', 'NN')]\n",
      "[('hai', 'NN')]\n",
      "[('jisay', 'NN')]\n",
      "[('saal', 'NN')]\n",
      "[('mein', 'NN')]\n",
      "[('sirf', 'NN')]\n",
      "[('1', 'CD')]\n",
      "[('baar', 'NN')]\n",
      "[('charge', 'NN')]\n",
      "[('karna', 'NN')]\n",
      "[('parre', 'NN')]\n",
      "[('ga', 'NN')]\n",
      "[('magar', 'NN')]\n",
      "[('yeh', 'NN')]\n",
      "[('battery', 'NN')]\n",
      "[('kamray', 'NN')]\n",
      "[('ke', 'NN')]\n",
      "[('daraja', 'NN')]\n",
      "[('hararat', 'NN')]\n",
      "[('mein', 'NN')]\n",
      "[('behtareen', 'NN')]\n",
      "[('kaam', 'NN')]\n",
      "[('kere', 'RB')]\n",
      "[('gi', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('aylominim', 'NN')]\n",
      "[('ion', 'NN')]\n",
      "[('battery', 'NN')]\n",
      "[('ko', 'NN')]\n",
      "[('jet', 'NN')]\n",
      "[('laboratory', 'NN')]\n",
      "[(',', ',')]\n",
      "[('larns', 'NNS')]\n",
      "[('national', 'JJ')]\n",
      "[('laboratory', 'NN')]\n",
      "[('Nasa', 'NN')]\n",
      "[(',', ',')]\n",
      "[('hnda', 'NN')]\n",
      "[('reserch', 'NN')]\n",
      "[('insti', 'NN')]\n",
      "[('tute', 'NN')]\n",
      "[('ke', 'NN')]\n",
      "[('mahireen', 'NN')]\n",
      "[('par', 'NN')]\n",
      "[('mushtamil', 'NN')]\n",
      "[('team', 'NN')]\n",
      "[('naay', 'NN')]\n",
      "[('tayyar', 'NN')]\n",
      "[('kya', 'NN')]\n",
      "[('hai', 'NN')]\n",
      "[('jisay', 'NN')]\n",
      "[('aik', 'NN')]\n",
      "[('karne', 'NN')]\n",
      "[('ke', 'NN')]\n",
      "[('baad', 'NN')]\n",
      "[('taqreeban', 'NN')]\n",
      "[('10', 'CD')]\n",
      "[('Mahtaq', 'NN')]\n",
      "[('dobarah', 'NN')]\n",
      "[('charge', 'NN')]\n",
      "[('karne', 'NN')]\n",
      "[('ki', 'NN')]\n",
      "[('zaroorat', 'NN')]\n",
      "[('nahi', 'NN')]\n",
      "[('rehti', 'NN')]\n",
      "[('hai', 'NN')]\n",
      "[('tehqeeqi', 'NN')]\n",
      "[('team', 'NN')]\n",
      "[('ke', 'NN')]\n",
      "[('sarbarah', 'NN')]\n",
      "[('aur', 'NN')]\n",
      "[('2008', 'CD')]\n",
      "[('hamza', 'NN')]\n",
      "[('mein', 'NN')]\n",
      "[('noble', 'JJ')]\n",
      "[('inaam', 'NN')]\n",
      "[('haasil', 'NN')]\n",
      "[('karne', 'NN')]\n",
      "[('walay', 'NN')]\n",
      "[('chemiya', 'NN')]\n",
      "[('ke', 'NN')]\n",
      "[('professor', 'NN')]\n",
      "[('Robert', 'NNP')]\n",
      "[('grbs', 'NNS')]\n",
      "[('ne', 'NN')]\n",
      "[('bataya', 'NN')]\n",
      "[('ke', 'NN')]\n",
      "[('aylominim', 'NN')]\n",
      "[('ion', 'NN')]\n",
      "[('battery', 'NN')]\n",
      "[('murawaja', 'NN')]\n",
      "[('batrion', 'NN')]\n",
      "[('ke', 'NN')]\n",
      "[('muqablay', 'NN')]\n",
      "[('mein', 'NN')]\n",
      "[('20', 'CD')]\n",
      "[('gina', 'NN')]\n",
      "[('taaqat', 'NN')]\n",
      "[('war', 'NN')]\n",
      "[('hoti', 'NN')]\n",
      "[('hain', 'NN')]\n",
      "[('aur', 'NN')]\n",
      "[('inhen', 'NN')]\n",
      "[('10', 'CD')]\n",
      "[('mah', 'NN')]\n",
      "[('taq', 'NN')]\n",
      "[('charge', 'NN')]\n",
      "[('karne', 'NN')]\n",
      "[('ki', 'NN')]\n",
      "[('zaroorat', 'NN')]\n",
      "[('nahi', 'NN')]\n",
      "[('padtee', 'NN')]\n",
      "[('ha', 'NN')]\n",
      "[('team', 'NN')]\n",
      "[('ke', 'NN')]\n",
      "[('sarbarah', 'NN')]\n",
      "[('professor', 'NN')]\n",
      "[('Robert', 'NNP')]\n",
      "[('grace', 'NN')]\n",
      "[('ke', 'NN')]\n",
      "[('mutabiq', 'NN')]\n",
      "[('aam', 'NN')]\n",
      "[('istemaal', 'NN')]\n",
      "[('ki', 'NN')]\n",
      "[('jany', 'NN')]\n",
      "[('wali', 'NN')]\n",
      "[('batrion', 'NN')]\n",
      "[('ke', 'NN')]\n",
      "[('muqablay', 'NN')]\n",
      "[('main', 'JJ')]\n",
      "[('zyada', 'NN')]\n",
      "[('achi', 'NNS')]\n",
      "[('h', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "Test2_token = word_tokenize(Test2)\n",
    "# Test2_token\n",
    "for token in Test2_token:\n",
    "    print(nltk.pos_tag([token]))\n",
    "#     tag = [i for i in nltk.pos_tag([token])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Sample: \"lekin daad dainee padtee hai snatkaron ko jo aisay bohat se digrmsayl ki terhan is maslay ka hal nikaalte mein bhi kamyaab ho gaye hain\", \"ghair mulki tehqeeqi jareeday fiction mein shaya honay wali report ke mutabiq snatkaron nay aik aisi battery tayaar karli hai jisay saal mein sirf 1 baar charge karna parre ga magar yeh battery kamray ke daraja hararat mein behtareen kaam kere gi\", \"is aylominim ion battery ko jet laboratory, larns national laboratory Nasa , hnda reserch insti tute ke mahireen par mushtamil team naay tayyar kya hai\", \"jisay aik karne ke baad taqreeban 10 Mah taq dobarah charge karne ki zaroorat nahi rehti hai\", \"tehqeeqi team ke sarbarah aur 2008 hamza mein noble inaam haasil karne walay chemiya ke professor Robert  grbs ne bataya ke aylominim ion battery murawaja batrion ke muqablay mein 20 gina taaqat war hoti hain aur inhen 10 mah taq charge karne ki zaroorat nahi padtee ha\", \"team ke sarbarah professor Robert grace ke mutabiq aam istemaal ki jany wali batrion ke muqablay main zyada achi h\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question2 miscellaneous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def cleaner(word):\n",
    "  word = re.sub(r'\\#\\.', '', word)\n",
    "  word = re.sub(r'\\n', '', word)\n",
    "  word = re.sub(r',', '', word)\n",
    "  word = re.sub(r'\\-', ' ', word)\n",
    "  word = re.sub(r'\\.', '', word)\n",
    "  word = re.sub(r'\\\\', ' ', word)\n",
    "  word = re.sub(r'\\\\x\\.+', '', word)\n",
    "  word = re.sub(r'\\d', '', word)\n",
    "  word = re.sub(r'^_.', '', word)\n",
    "  word = re.sub(r'_', ' ', word)\n",
    "  word = re.sub(r'^ ', '', word)\n",
    "  word = re.sub(r' $', '', word)\n",
    "  word = re.sub(r'\\?', '', word)\n",
    "\n",
    "  return word.lower()\n",
    "\n",
    "\n",
    "def hashing(word):\n",
    "  word = re.sub(r'ain$', r'ein', word)\n",
    "  word = re.sub(r'ai', r'ae', word)\n",
    "  word = re.sub(r'ay$', r'e', word)\n",
    "  word = re.sub(r'ey$', r'e', word)\n",
    "  word = re.sub(r'ie$', r'y', word)\n",
    "  word = re.sub(r'^es', r'is', word)\n",
    "  word = re.sub(r'a+', r'a', word)\n",
    "  word = re.sub(r'j+', r'j', word)\n",
    "  word = re.sub(r'd+', r'd', word)\n",
    "  word = re.sub(r'u', r'o', word)\n",
    "  word = re.sub(r'o+', r'o', word)\n",
    "  word = re.sub(r'ee+', r'i', word)\n",
    "  if not re.match(r'ar', word):\n",
    "    word = re.sub(r'ar', r'r', word)\n",
    "  word = re.sub(r'iy+', r'i', word)\n",
    "  word = re.sub(r'ih+', r'eh', word)\n",
    "  word = re.sub(r's+', r's', word)\n",
    "  if re.search(r'[rst]y', 'word') and word[-1] != 'y':\n",
    "    word = re.sub(r'y', r'i', word)\n",
    "  if re.search(r'[bcdefghijklmnopqrtuvwxyz]i', word):\n",
    "    word = re.sub(r'i$', r'y', word)\n",
    "  if re.search(r'[acefghijlmnoqrstuvwxyz]h', word):\n",
    "    word = re.sub(r'h', '', word)\n",
    "  word = re.sub(r'k', r'q', word)\n",
    "  return word\n",
    "\n",
    "def array_cleaner(array):\n",
    "  # X = array\n",
    "  X = []\n",
    "  for sentence in array:\n",
    "    clean_sentence = ''\n",
    "    words = str(sentence).split(' ')\n",
    "    for word in words:\n",
    "      clean_sentence = clean_sentence +' '+ cleaner(word)\n",
    "    X.append(clean_sentence)\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leqin dad daeni padti ae snatqron qo jo aesay boat se digrmsayl qi teran is maslay qa al niqalte mein bi qamyab o gaye aen gaer molqi teqiqi jriday fiction mein saya onay wali report qe motabiq snatqron nay aeq aesi battery tayr qrli ae jisay sal mein sirf 1 br crge qrna prre ga magr ye battery qamray qe draja rrat mein betrin qam qere gi is aylominim ion battery qo jet laboratory, lrns national laboratory Nasa , nda reserc insti tote qe mairin pr mostamil team nay tayyr qya ae jisay aeq qrne qe bad taqriban 10 Mataq dobra crge qrne qi zrorat nai reti ae teqiqi team qe srbra aor 2008 amza mein noble inam asil qrne walay cemia qe profesor Robert  grbs ne bataya qe aylominim ion battery morawaja batrion qe moqablay mein 20 gina taqat wr oti aen aor inen 10 ma taq crge qrne qi zrorat nai padti a team qe srbra profesor Robert grace qe motabiq am istemal qi jany wali batrion qe moqablay maen zyada aci '"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner(Test2)\n",
    "hashing(Test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Q3: Find the longest word in Test2 and that word's length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'laboratory': 10}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "Test2_token = word_tokenize(Test2)\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "fd = FreqDist()\n",
    "for word in Test2_token:\n",
    "    fd[word]+=1\n",
    "fd.max() # most common\n",
    "\n",
    "\n",
    "words = [word for word in Test2_token]\n",
    "longest_string ={max(words, key=len):len(max(words, key=len))}\n",
    "longest_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Rule Based Roman Urdu Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Roman Urdu lacks standard lexicon and usually many spelling variations exist for a given word, e.g., the word\n",
    "zindagi (life) is also written as zindagee, zindagy, zaindagee and zndagi. So, in this question you have to Normalize Roman Urdu words using the following Rules given in the attached Pdf. Your Code works for a complete Sentence or multiple sentences.\n",
    "\n",
    "For Example: zaroori, zaruri, zarori map to the 'zrory'. So zrory becomes the correct word for all representations mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
